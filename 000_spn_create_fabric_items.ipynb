{"cells":[{"cell_type":"markdown","source":["### 000_spn_create_fabric_items\n","\n","#### Creates Microsoft Fabric workspace items (Lakehouse, Notebook, Warehouse, Pipeline, Dataflow Gen2) via REST API using Service Principal authentication.\n","##### Avoids pesky object staleness issues associated with users leaving the organization / not logging in for a while!\n","\n","Prerequisites:\n"," - Service Principal with Fabric API permissions\n"," - SPN added as Workspace Admin (or Contributor) in target workspace\n"," - Key Vault access (if using KV-based credential retrieval, recommended)\n","\n","Usage:\n","1. Configure credentials in CONFIGURATION section (Key Vault OR manual)\n","2. Run all cells through \"Core Functions\"\n","3. Uncomment/call desired create_* functions in \"Execution\" section"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"08fbcf18-a457-4ea4-bc8d-1074b0379249"},{"cell_type":"code","source":["import requests\n","from typing import Optional\n","import notebookutils.credentials as cred"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cd8409c6-b5d6-4a85-8440-a90007d05f7f"},{"cell_type":"markdown","source":["### Configuration"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"332a1841-f277-4cb7-9a19-c2b9ef9f224d"},{"cell_type":"code","source":["# Choose ONE authentication method: Key Vault (recommended) OR Manual\n","\n","# -----------------------------------------------------------------------------\n","# Option A: Key Vault-based credentials (RECOMMENDED for production)\n","# -----------------------------------------------------------------------------\n","# Uncomment and configure the following block to use Key Vault:\n","#\n","\n","# KEY_VAULT_NAME = \"your-keyvault-name\"  # e.g., \"kv-fabric-prod\"\n","# KEY_VAULT_URL = f\"https://{KEY_VAULT_NAME}.vault.azure.net/\"\n","#\n","# TENANT_ID = cred.getSecret(KEY_VAULT_URL, \"aad-tenant-id\")\n","# SP_CLIENT_ID = cred.getSecret(KEY_VAULT_URL, \"fabric-spn-client-id\")\n","# SP_CLIENT_SECRET = cred.getSecret(KEY_VAULT_URL, \"fabric-spn-client-secret\")\n","\n","# -----------------------------------------------------------------------------\n","# Option B: Manual credentials (for development/testing ONLY)\n","# -----------------------------------------------------------------------------\n","# WARNING: Never commit credentials to source control!\n","TENANT_ID = \"\"          # Azure AD Tenant ID\n","SP_CLIENT_ID = \"\"       # Service Principal Application (Client) ID\n","SP_CLIENT_SECRET = \"\"   # Service Principal Client Secret\n","\n","# -----------------------------------------------------------------------------\n","# Workspace Configuration (auto-detected from runtime context)\n","# -----------------------------------------------------------------------------\n","_context = mssparkutils.runtime.context\n","WORKSPACE_ID = _context[\"currentWorkspaceId\"]\n","print(f\"Target Workspace ID: {WORKSPACE_ID}\")\n","\n","# -----------------------------------------------------------------------------\n","# API Configuration\n","# -----------------------------------------------------------------------------\n","FABRIC_API_BASE_URL = \"https://api.fabric.microsoft.com/v1\"\n","OAUTH_TOKEN_URL_TEMPLATE = \"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n","FABRIC_RESOURCE_URI = \"https://api.fabric.microsoft.com\""],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"68063dfc-ff36-4db5-b74b-89c507149605"},{"cell_type":"markdown","source":["### Core Functions"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"81b5bc41-12c4-462b-a1b7-1bfcff92d5a6"},{"cell_type":"code","source":["# =============================================================================\n","# AUTHENTICATION\n","# =============================================================================\n","def get_access_token(\n","    tenant_id: str,\n","    client_id: str,\n","    client_secret: str\n",") -> str:\n","    \"\"\"\n","    Acquire OAuth2 access token from Microsoft Entra ID using client credentials flow.\n","    \n","    This function authenticates the Service Principal against Microsoft's OAuth2\n","    token endpoint and returns a bearer token for Fabric API calls.\n","    \n","    Args:\n","        tenant_id: Azure AD tenant identifier (GUID)\n","        client_id: Service Principal application (client) ID\n","        client_secret: Service Principal client secret value\n","    \n","    Returns:\n","        str: Bearer access token for Fabric API authentication\n","    \n","    Raises:\n","        requests.HTTPError: If authentication fails (invalid credentials, \n","                           insufficient permissions, etc.)\n","        KeyError: If response doesn't contain expected 'access_token' field\n","    \n","    Example:\n","        >>> token = get_access_token(TENANT_ID, SP_CLIENT_ID, SP_CLIENT_SECRET)\n","        >>> print(f\"Token acquired: {token[:20]}...\")\n","    \"\"\"\n","    token_url = OAUTH_TOKEN_URL_TEMPLATE.format(tenant_id=tenant_id)\n","    \n","    headers = {\n","        \"Content-Type\": \"application/x-www-form-urlencoded\"\n","    }\n","    \n","    payload = {\n","        \"grant_type\": \"client_credentials\",\n","        \"client_id\": client_id,\n","        \"client_secret\": client_secret,\n","        \"resource\": FABRIC_RESOURCE_URI\n","    }\n","    \n","    response = requests.post(token_url, data=payload, headers=headers)\n","    response.raise_for_status()\n","    \n","    token_data = response.json()\n","    access_token = token_data[\"access_token\"]\n","    \n","    return access_token\n","\n","\n","# =============================================================================\n","# HELPER FUNCTIONS\n","# =============================================================================\n","def _get_auth_headers(access_token: str) -> dict:\n","    \"\"\"\n","    Construct standard authorization headers for Fabric API requests.\n","    \n","    Args:\n","        access_token: Valid OAuth2 bearer token\n","    \n","    Returns:\n","        dict: Headers dictionary with Authorization and Content-Type\n","    \"\"\"\n","    return {\n","        \"Authorization\": f\"Bearer {access_token}\",\n","        \"Content-Type\": \"application/json\"\n","    }\n","\n","\n","def _make_fabric_request(\n","    access_token: str,\n","    endpoint_path: str,\n","    payload: dict,\n","    item_type: str,\n","    item_name: str\n",") -> dict:\n","    \"\"\"\n","    Execute a POST request to the Fabric REST API with standardized error handling.\n","    \n","    Args:\n","        access_token: Valid OAuth2 bearer token\n","        endpoint_path: API endpoint path (appended to FABRIC_API_BASE_URL)\n","        payload: JSON request body\n","        item_type: Human-readable item type for logging (e.g., \"Lakehouse\")\n","        item_name: Display name of item being created\n","    \n","    Returns:\n","        dict: API response JSON (empty dict if 202 Accepted with no body)\n","    \n","    Raises:\n","        requests.HTTPError: If API returns error status code\n","    \"\"\"\n","    url = f\"{FABRIC_API_BASE_URL}{endpoint_path}\"\n","    headers = _get_auth_headers(access_token)\n","    \n","    response = requests.post(url, headers=headers, json=payload)\n","    \n","    # 201 = Created, 202 = Accepted (long-running provisioning)\n","    response.raise_for_status()\n","    \n","    status_msg = \"created\" if response.status_code == 201 else \"provisioning started\"\n","    print(f\"[SUCCESS] {item_type} '{item_name}' {status_msg} (HTTP {response.status_code})\")\n","    \n","    # Some endpoints return empty body on 202\n","    if response.content:\n","        return response.json()\n","    return {}\n","\n","\n","# =============================================================================\n","# CORE FUNCTIONS - FABRIC ITEM CREATION\n","# =============================================================================\n","def create_lakehouse(\n","    access_token: str,\n","    lakehouse_name: str,\n","    enable_schemas: bool = False\n",") -> dict:\n","    \"\"\"\n","    Create a Microsoft Fabric Lakehouse in the current workspace.\n","    \n","    Lakehouses combine data lake storage with relational database capabilities.\n","    Schema-enabled lakehouses support database schemas for organizing tables.\n","    \n","    Args:\n","        access_token: OAuth2 bearer token from Entra ID\n","        lakehouse_name: Display name for the Lakehouse (must be unique in workspace)\n","        enable_schemas: When True, provisions a schema-enabled Lakehouse (preview).\n","                       Schema-enabled lakehouses support CREATE SCHEMA DDL.\n","    \n","    Returns:\n","        dict: API response containing created item metadata\n","    \n","    Raises:\n","        requests.HTTPError: If creation fails (duplicate name, permissions, etc.)\n","    \n","    Example:\n","        >>> create_lakehouse(token, \"lh_bronze\", enable_schemas=True)\n","        [SUCCESS] Lakehouse 'lh_bronze' created (HTTP 201)\n","    \"\"\"\n","    endpoint = f\"/workspaces/{WORKSPACE_ID}/lakehouses\"\n","    \n","    payload = {\n","        \"displayName\": lakehouse_name\n","    }\n","    \n","    # Schema-enabled lakehouse (preview feature)\n","    if enable_schemas:\n","        payload[\"creationPayload\"] = {\"enableSchemas\": True}\n","    \n","    return _make_fabric_request(\n","        access_token=access_token,\n","        endpoint_path=endpoint,\n","        payload=payload,\n","        item_type=\"Lakehouse\",\n","        item_name=lakehouse_name\n","    )\n","\n","\n","def create_notebook(access_token: str, notebook_name: str) -> dict:\n","    \"\"\"\n","    Create an empty Fabric Notebook in the current workspace.\n","    \n","    Creates a new PySpark notebook with default configuration. The notebook\n","    will be empty and ready for development.\n","    \n","    Args:\n","        access_token: OAuth2 bearer token from Entra ID\n","        notebook_name: Display name for the Notebook (must be unique in workspace)\n","    \n","    Returns:\n","        dict: API response containing created item metadata\n","    \n","    Raises:\n","        requests.HTTPError: If creation fails\n","    \n","    Example:\n","        >>> create_notebook(token, \"nb_100_ingest_raw_data\")\n","        [SUCCESS] Notebook 'nb_100_ingest_raw_data' created (HTTP 201)\n","    \"\"\"\n","    endpoint = f\"/workspaces/{WORKSPACE_ID}/notebooks\"\n","    \n","    payload = {\n","        \"displayName\": notebook_name\n","    }\n","    \n","    return _make_fabric_request(\n","        access_token=access_token,\n","        endpoint_path=endpoint,\n","        payload=payload,\n","        item_type=\"Notebook\",\n","        item_name=notebook_name\n","    )\n","\n","\n","def create_warehouse(access_token: str, warehouse_name: str) -> dict:\n","    \"\"\"\n","    Create a Fabric Warehouse in the current workspace.\n","    \n","    Warehouses provide T-SQL-based analytics with automatic scaling.\n","    Note: Warehouse provisioning may take several minutes.\n","    \n","    Args:\n","        access_token: OAuth2 bearer token from Entra ID\n","        warehouse_name: Display name for the Warehouse (must be unique in workspace)\n","    \n","    Returns:\n","        dict: API response containing created item metadata\n","    \n","    Raises:\n","        requests.HTTPError: If creation fails\n","    \n","    Example:\n","        >>> create_warehouse(token, \"wh_gold\")\n","        [SUCCESS] Warehouse 'wh_gold' provisioning started (HTTP 202)\n","    \"\"\"\n","    endpoint = f\"/workspaces/{WORKSPACE_ID}/warehouses\"\n","    \n","    payload = {\n","        \"displayName\": warehouse_name\n","    }\n","    \n","    return _make_fabric_request(\n","        access_token=access_token,\n","        endpoint_path=endpoint,\n","        payload=payload,\n","        item_type=\"Warehouse\",\n","        item_name=warehouse_name\n","    )\n","\n","\n","def create_pipeline(access_token: str, pipeline_name: str) -> dict:\n","    \"\"\"\n","    Create a Fabric Data Pipeline in the current workspace.\n","    \n","    Data Pipelines orchestrate data movement and transformation activities.\n","    The created pipeline will be empty and ready for activity configuration.\n","    \n","    Args:\n","        access_token: OAuth2 bearer token from Entra ID\n","        pipeline_name: Display name for the Pipeline (must be unique in workspace)\n","    \n","    Returns:\n","        dict: API response containing created item metadata\n","    \n","    Raises:\n","        requests.HTTPError: If creation fails\n","    \n","    Example:\n","        >>> create_pipeline(token, \"pl_100_daily_refresh\")\n","        [SUCCESS] Pipeline 'pl_100_daily_refresh' created (HTTP 201)\n","    \"\"\"\n","    endpoint = f\"/workspaces/{WORKSPACE_ID}/dataPipelines\"\n","    \n","    payload = {\n","        \"displayName\": pipeline_name\n","    }\n","    \n","    return _make_fabric_request(\n","        access_token=access_token,\n","        endpoint_path=endpoint,\n","        payload=payload,\n","        item_type=\"Pipeline\",\n","        item_name=pipeline_name\n","    )\n","\n","\n","def create_dataflow_gen2(access_token: str, dataflow_name: str) -> dict:\n","    \"\"\"\n","    Create a Fabric Dataflow Gen2 in the current workspace.\n","    \n","    Dataflow Gen2 provides Power Query-based data transformation with \n","    enhanced performance and Fabric integration.\n","    \n","    Args:\n","        access_token: OAuth2 bearer token from Entra ID\n","        dataflow_name: Display name for the Dataflow (must be unique in workspace)\n","    \n","    Returns:\n","        dict: API response containing created item metadata\n","    \n","    Raises:\n","        requests.HTTPError: If creation fails\n","    \n","    Example:\n","        >>> create_dataflow_gen2(token, \"df_transform_customers\")\n","        [SUCCESS] Dataflow Gen2 'df_transform_customers' created (HTTP 201)\n","    \"\"\"\n","    endpoint = f\"/workspaces/{WORKSPACE_ID}/dataflows\"\n","    \n","    payload = {\n","        \"displayName\": dataflow_name\n","    }\n","    \n","    return _make_fabric_request(\n","        access_token=access_token,\n","        endpoint_path=endpoint,\n","        payload=payload,\n","        item_type=\"Dataflow Gen2\",\n","        item_name=dataflow_name\n","    )\n","\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9b6656d7-315d-49b3-8305-019c57ed49a4"},{"cell_type":"code","source":["# Acquire access token (required before creating any items)\n","access_token = get_access_token(TENANT_ID, SP_CLIENT_ID, SP_CLIENT_SECRET)\n","print(\"Access token acquired successfully.\\n\")\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c3752958-06bd-42c8-997f-ed2f8853ea3a"},{"cell_type":"markdown","source":["### Execution"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6bbf2e22-9855-4a9d-9c64-8374d91c4b68"},{"cell_type":"code","source":["# -----------------------------------------------------------------------------\n","# Create Fabric items by uncommenting desired lines below\n","# -----------------------------------------------------------------------------\n","\n","# Lakehouses (set enable_schemas=True for schema-enabled lakehouse)\n","# create_lakehouse(access_token, \"lh_bronze\", enable_schemas=True)\n","# create_lakehouse(access_token, \"lh_silver\", enable_schemas=True)\n","# create_lakehouse(access_token, \"lh_gold\", enable_schemas=True)\n","\n","# Notebooks\n","# create_notebook(access_token, \"notebook_name\")\n","\n","# Warehouses\n","# create_warehouse(access_token, \"wh_name\")\n","\n","# Pipelines\n","# create_pipeline(access_token, \"pl_name\")\n","\n","# Dataflows\n","# create_dataflow_gen2(access_token, \"df__name\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"45498214-fdac-4729-97e6-8778bec15ebc"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}